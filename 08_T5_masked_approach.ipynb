{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd150bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (T5Tokenizer, \n",
    "                          T5Config, \n",
    "                          T5ForConditionalGeneration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cc18278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "T5_PATH = 't5-base' # \"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"\n",
    "DEVICE = 'cpu' #torch.device('cuda' if torch.cuda.is_available() else 'cpu') # My envirnment uses CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf75966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(T5_PATH)\n",
    "t5_config = T5Config.from_pretrained(T5_PATH)\n",
    "t5_mlm = T5ForConditionalGeneration.from_pretrained(T5_PATH, config=t5_config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "77658bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "text = '''Litwo ojczyzno moja ty jesteś jak zdrowie\n",
    "<extra_id_0> <extra_id_1> <extra_id_2> posłowie</s>'''\n",
    "\n",
    "text = '''O Lithuania, my native land,\n",
    "you are like health--so valued when lost\n",
    "beyond recovery; let these words now stand\n",
    "<extra_id_0> cost.</s>\n",
    "'''\n",
    "\n",
    "encoded = t5_tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt')\n",
    "input_ids = encoded['input_ids'].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dbc53d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = t5_mlm.generate(input_ids=input_ids, \n",
    "                          num_beams=200, num_return_sequences=1,\n",
    "                          max_length=50)\n",
    "\n",
    "_0_index = text.index('<extra_id_0>')\n",
    "_result_prefix = text[:_0_index]\n",
    "# _2_index = text.index('<extra_id_2>')\n",
    "_result_suffix = text[_0_index+12:]  # 12 is the length of <extra_id_0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1cce3a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Lithuania, my native land,\n",
      "you are like health--so valued when lost\n",
      "beyond recovery; let these words now stand\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(_result_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "31d3c048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <extra_id_0> the<extra_id_1> the cost. O Lithuania, my native land, you are like health--so valued when lost beyond recovery, so valued when lost beyond recovery, so valued when lost beyond recovery, so valued when lost beyond recovery, so\n"
     ]
    }
   ],
   "source": [
    "txt = t5_tokenizer.decode(outputs[0], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c31bab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cost.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(_result_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0b0cca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Lithuania, my native land,\n",
      "you are like health--so valued when lost\n",
      "beyond recovery; let these words now stand\n",
      "the cost.</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def _filter(output, end_token='<extra_id_1>'):\n",
    "    # The first token is <unk> (inidex at 0) and the second token is <extra_id_0> (indexed at 32099)\n",
    "    _txt = t5_tokenizer.decode(output[2:], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    if end_token in _txt:\n",
    "        _end_token_index = _txt.index(end_token)\n",
    "        return _result_prefix + _txt[:_end_token_index] + _result_suffix\n",
    "    else:\n",
    "        return _result_prefix + _txt + _result_suffix\n",
    "\n",
    "results = list(map(_filter, outputs))\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c0a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
