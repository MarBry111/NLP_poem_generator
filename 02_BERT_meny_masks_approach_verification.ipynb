{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "455ca794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from transformers import HerbertTokenizer, RobertaModel\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForNextSentencePrediction\n",
    "\n",
    "from transformers import pipeline\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef4df9",
   "metadata": {},
   "source": [
    "## BERT with many masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adbca2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.huggingface.co/t/multiple-mask-tokens/174/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dee9988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "input_txt = \"[MASK] [MASK] [MASK] what should be done next, even it was not easy.\"\n",
    "inputs = tokenizer_en(input_txt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "925c0e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_en = BertForMaskedLM.from_pretrained('bert-base-cased')\n",
    " \n",
    "outputs = model_en(**inputs)\n",
    "predictions = outputs[0]\n",
    "sorted_preds, sorted_idx = predictions[0].sort(dim=-1, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcbeee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = sorted_preds.shape[0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1515aca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He knew knew what should be done next , even it was not easy .\n"
     ]
    }
   ],
   "source": [
    "predicted_index = [sorted_idx[i, 0].item() for i in range(0,m)]\n",
    "predicted_token = [tokenizer_en.convert_ids_to_tokens([predicted_index[x]])[0] for x in range(1,m)]\n",
    "print(' '.join(predicted_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9f783d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9999e-01, 1.0484e-05]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_nxt_en = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "prompt = \"The child came home from school.\"\n",
    "next_sentence = \"He played soccer after school.\"\n",
    "\n",
    "encoding = tokenizer_en.encode_plus(prompt, next_sentence, return_tensors='pt')\n",
    "\n",
    "outputs = model_nxt_en(**encoding)[0]\n",
    "softmax = F.softmax(outputs, dim = 1)\n",
    "print(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b875c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0484e-05, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax[0,1] / softmax[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3358498",
   "metadata": {},
   "source": [
    "### Polish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ababa785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/dkleczek/bert-base-polish-uncased-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "19376de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dkleczek/bert-base-polish-cased-v1 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_pl = BertForMaskedLM.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\")\n",
    "tokenizer_pl = BertTokenizer.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\")\n",
    "nlp = pipeline('fill-mask', model=model_pl, tokenizer=tokenizer_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9db261d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"Adam [MASK] wielkim polskim [MASK] był.\"\n",
    "inputs = tokenizer_pl(input_txt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7a8a4d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model_pl(**inputs)\n",
    "predictions = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15279dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam II wielkim polskim człowiekiem był .\n"
     ]
    }
   ],
   "source": [
    "sorted_preds, sorted_idx = predictions[0].sort(dim=-1, descending=True)\n",
    "m = sorted_preds.shape[0] - 1\n",
    "predicted_index = [sorted_idx[i, 0].item() for i in range(0,m)]\n",
    "predicted_token = [tokenizer_pl.convert_ids_to_tokens([predicted_index[x]])[0] for x in range(1,m)]\n",
    "print(' '.join(predicted_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f2bd4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dkleczek/bert-base-polish-uncased-v1 were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9733, 0.0267]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_nxt_pl = BertForNextSentencePrediction.from_pretrained('dkleczek/bert-base-polish-uncased-v1')\n",
    "\n",
    "prompt = \"Nad rzeczką opodal krzaczka.\"\n",
    "next_sentence = \"Mieszkała kaczka-dziwaczka\"\n",
    "\n",
    "encoding = tokenizer_pl.encode_plus(prompt, next_sentence, return_tensors='pt')\n",
    "\n",
    "outputs = model_nxt_pl(**encoding)[0]\n",
    "softmax = F.softmax(outputs, dim = 1)\n",
    "print(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0bbfa56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small values - good\n",
    "# big first one and small second one - means those sentences should go one after another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e65e7fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0274, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax[0,1] / softmax[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4dfcaeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9983, 0.0017]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "tensor(0.0017, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Zygmunt III Waza był wszechstronnie wykształcony, biegle władał pięcioma językami, w tym językiem polskim.\"\n",
    "next_sentence = \"Był dobrym gospodarzem, trzykrotnie zwiększył dochody skarbu królewskiego.\"\n",
    "\n",
    "encoding = tokenizer_pl.encode_plus(prompt, next_sentence, return_tensors='pt')\n",
    "\n",
    "outputs = model_nxt_pl(**encoding)[0]\n",
    "softmax = F.softmax(outputs, dim = 1)\n",
    "print(softmax, '\\n')\n",
    "\n",
    "print(softmax[0,1] / softmax[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b13abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
