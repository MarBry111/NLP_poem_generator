{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455ca794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from transformers import HerbertTokenizer, RobertaModel\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForNextSentencePrediction\n",
    "\n",
    "from transformers import pipeline\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef4df9",
   "metadata": {},
   "source": [
    "## BERT with many masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4ec93",
   "metadata": {},
   "source": [
    "Checking approach with [MASK] tokens and known last word of next sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dee9988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "input_txt = \"[MASK] [MASK] [MASK] what should be done next, even it was not easy.\"\n",
    "inputs = tokenizer_en(input_txt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "925c0e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_en = BertForMaskedLM.from_pretrained('bert-base-cased')\n",
    " \n",
    "outputs = model_en(**inputs)\n",
    "predictions = outputs[0]\n",
    "sorted_preds, sorted_idx = predictions[0].sort(dim=-1, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcbeee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = sorted_preds.shape[0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1515aca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He knew knew what should be done next , even it was not easy .\n"
     ]
    }
   ],
   "source": [
    "predicted_index = [sorted_idx[i, 0].item() for i in range(0,m)]\n",
    "predicted_token = [tokenizer_en.convert_ids_to_tokens([predicted_index[x]])[0] for x in range(1,m)]\n",
    "print(' '.join(predicted_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b83d9af",
   "metadata": {},
   "source": [
    "Determine if next sentence is really the following one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9f783d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9999e-01, 1.0484e-05]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_nxt_en = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "prompt = \"The child came home from school.\"\n",
    "next_sentence = \"He played soccer after school.\"\n",
    "\n",
    "encoding = tokenizer_en.encode_plus(prompt, next_sentence, return_tensors='pt')\n",
    "\n",
    "outputs = model_nxt_en(**encoding)[0]\n",
    "softmax = F.softmax(outputs, dim = 1)\n",
    "print(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b875c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0484e-05, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax[0,1] / softmax[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3358498",
   "metadata": {},
   "source": [
    "### Polish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19376de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dkleczek/bert-base-polish-cased-v1 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import HerbertTokenizer, RobertaModel\n",
    "\n",
    "model_pl = BertForMaskedLM.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\")\n",
    "tokenizer_pl = BertTokenizer.from_pretrained(\"dkleczek/bert-base-polish-cased-v1\")\n",
    "\n",
    "# model_pl = BertForMaskedLM.from_pretrained(\"allegro/herbert-klej-cased-v1\")\n",
    "# tokenizer_pl = HerbertTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")\n",
    "nlp = pipeline('fill-mask', model=model_pl, tokenizer=tokenizer_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9db261d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"\"\"Litwo Ojczyzno moja ty jesteś jak zdrowie. Tak mi przytnę uszy Twe,\n",
    "Żeś mię sławił, g’woli Twojej wiernego [MASK].\"\"\"\n",
    "inputs = tokenizer_pl(input_txt, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21ca82e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  8007,   832, 30958, 40240,  2797,   882,  1285,   848, 11031,\n",
       "            18,  1018,   897, 27299,  1114, 18654, 27142,    16,  3178,   464,\n",
       "          4574, 15232,  9489,    16,    75,   364,  7992, 12987,  4959,   912,\n",
       "             3,    18,     4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5928ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Litwo Ojczyzno moja ty jesteś jak zdrowie. Tak mi przytnę uszy Twe, Żeś mię sławił, g ’ woli Twojej wiernego [MASK]. [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_pl.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a8a4d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model_pl(inputs['input_ids'])\n",
    "predictions = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15279dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Litwo Ojczyzno moja ty jesteś jak zdrowie . Tak mi przytnę uszy Twe , Żeś mię sławił , g ’ woli Twojej wiernego Boga .\n"
     ]
    }
   ],
   "source": [
    "sorted_preds, sorted_idx = predictions[0].sort(dim=-1, descending=True)\n",
    "m = sorted_preds.shape[0] - 1\n",
    "predicted_index = [sorted_idx[i, 0].item() for i in range(0,m)]\n",
    "predicted_token = [tokenizer_pl.convert_ids_to_tokens([predicted_index[x]])[0] for x in range(1,m)]\n",
    "print(' '.join(predicted_token).replace(' ##', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f647a9ad",
   "metadata": {},
   "source": [
    "Following of the next sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f2bd4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dkleczek/bert-base-polish-uncased-v1 were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9733, 0.0267]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_nxt_pl = BertForNextSentencePrediction.from_pretrained('dkleczek/bert-base-polish-uncased-v1')\n",
    "\n",
    "prompt = \"Nad rzeczką opodal krzaczka.\"\n",
    "next_sentence = \"Mieszkała kaczka-dziwaczka\"\n",
    "\n",
    "encoding = tokenizer_pl.encode_plus(prompt, next_sentence, return_tensors='pt')\n",
    "\n",
    "outputs = model_nxt_pl(**encoding)[0]\n",
    "softmax = F.softmax(outputs, dim = 1)\n",
    "print(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bbfa56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small values - good\n",
    "# big first one and small second one - means those sentences should go one after another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e65e7fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0274, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax[0,1] / softmax[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dfcaeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9983, 0.0017]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "tensor(0.0017, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Zygmunt III Waza był wszechstronnie wykształcony, biegle władał pięcioma językami, w tym językiem polskim.\"\n",
    "next_sentence = \"Był dobrym gospodarzem, trzykrotnie zwiększył dochody skarbu królewskiego.\"\n",
    "\n",
    "encoding = tokenizer_pl.encode_plus(prompt, next_sentence, return_tensors='pt')\n",
    "\n",
    "outputs = model_nxt_pl(**encoding)[0]\n",
    "softmax = F.softmax(outputs, dim = 1)\n",
    "print(softmax, '\\n')\n",
    "\n",
    "print(softmax[0,1] / softmax[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b13abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070cf8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
