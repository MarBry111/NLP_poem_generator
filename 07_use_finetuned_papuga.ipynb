{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "TIgckz6CYC3r",
   "metadata": {
    "id": "TIgckz6CYC3r"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389caad3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "389caad3",
    "outputId": "83480a68-89be-4f83-a7f9-040f6a9f911c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import re\n",
    "\n",
    "import pyphen\n",
    "import jellyfish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473ca71",
   "metadata": {},
   "source": [
    "Object to divide word to syllabes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "935d8ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = pyphen.Pyphen(lang='pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7f38b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a7f38b4",
    "outputId": "857bfae6-41fb-43b1-d6be-8d6b2b9c09c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/marek/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:907: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained('gpt2-xl',\n",
    "#                                           bos_token='<|startoftext|>', \n",
    "#                                           eos_token='<|endoftext|>', \n",
    "#                                           pad_token='<|pad|>')\n",
    "\n",
    "# model = AutoModelWithLMHead.from_pretrained('gpt2-xl')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('flax-community/papuGaPT2',\n",
    "                                          bos_token='<|startoftext|>', \n",
    "                                          eos_token='<|endoftext|>', \n",
    "                                          pad_token='<|pad|>')\n",
    "\n",
    "# model = AutoModelWithLMHead.from_pretrained('models/papuga-pantadeusz')\n",
    "model = AutoModelWithLMHead.from_pretrained('models/papuga-poems-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gaZQVH5mTo2x",
   "metadata": {
    "id": "gaZQVH5mTo2x"
   },
   "source": [
    "## Verify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Qb81QzYxR73j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qb81QzYxR73j",
    "outputId": "7b932bad-6a67-47bc-8432-05cbb7cc8b7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Kazimierze, kazimierze,\n",
      "Zbyszkom przystało, paniom starym\n",
      "Grzechem tedy niewdzięczność jest dana;\n",
      "Toż z tej wstyd żony, to z niej;\n",
      "Bystremu nie żalowi, to z niej;\n",
      "Wychodzi więc, że z niej cnota,\n",
      "Przypatrzże się, Kazimierze, tobie,\n",
      "To radość frasunek familijnej dzialności.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Litwo Ojczyzno moja ty jesteś jak zdrowie\\n\"\n",
    "prompt = \"<|startoftext|>\"\n",
    "\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "    generated,\n",
    "    do_sample=True, \n",
    "    max_length=200, \n",
    "    top_k=50, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33h3Rh_MNqRq",
   "metadata": {
    "id": "33h3Rh_MNqRq"
   },
   "outputs": [],
   "source": [
    "model_g = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hYZLWwEGUcnY",
   "metadata": {
    "id": "hYZLWwEGUcnY"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "generated_num = 0\n",
    "generated_list = []\n",
    "\n",
    "filter_value = -float(\"Inf\")\n",
    "\n",
    "prompt = \"Litwo Ojczyzno moja ty jesteś jak zdrowie\\n\"\n",
    "\n",
    "entry_count=10\n",
    "entry_length=200\n",
    "top_p=0.85 #sort word probabilities descending order, sum upp to p dropping other words, keeping few\n",
    "temperature=1. # higher more originlal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dWcX7vAdNmJG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "dWcX7vAdNmJG",
    "outputId": "f986522d-6986-4dc1-c5aa-1783e76cd1ec"
   },
   "outputs": [],
   "source": [
    "def get_next_token(generated, n_logits=None):\n",
    "    outputs = model_g(generated, labels=generated)\n",
    "    loss, logits = outputs[:2]\n",
    "    logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    \n",
    "    if n_logits:\n",
    "        f_softmax = F.softmax(logits, dim=-1)\n",
    "        indeces_sort = torch.argsort(f_softmax)[0, :n_logits]\n",
    "    \n",
    "    logits[:, indices_to_remove] = filter_value\n",
    "    next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "        \n",
    "    if n_logits:\n",
    "        return next_token, indeces_sort \n",
    "    else:\n",
    "        return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af534482",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_r = 'aabb'\n",
    "if type_r == 'aabb':\n",
    "    wersy_n = 1\n",
    "else:\n",
    "    wersy_n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b84ab6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_word(prev_word, possible_endings):\n",
    "    prev_syllab = ''.join(dic.inserted(prev_word).split('-')[-2:])\n",
    "\n",
    "    simmilarities = []\n",
    "    possible_endings = np.array(possible_endings)\n",
    "\n",
    "    for p in possible_endings:\n",
    "        last_word = re.sub(\"[^0-9a-zA-Z\\s]+\", '', p).strip().split(' ')[-1]\n",
    "        last_syllab = ''.join(dic.inserted(last_word).split('-')[-2:])\n",
    "        \n",
    "        min_l = np.min( [len(last_syllab), len(prev_syllab)] )\n",
    "        \n",
    "        if min_l > 1:\n",
    "            prev_syllab_tmp = prev_word[-min_l:].lower()\n",
    "            last_syllab_tmp = last_word[-min_l:].lower()\n",
    "            s = jellyfish.levenshtein_distance(prev_syllab_tmp, last_syllab_tmp)\n",
    "            simmilarities.append(s/min_l)\n",
    "        else:\n",
    "            simmilarities.append(10)\n",
    "\n",
    "    choose_endings = possible_endings[np.min(simmilarities) == np.array(simmilarities)]\n",
    "    ending = choose_endings[np.random.randint(0,choose_endings.shape[0])]\n",
    "    \n",
    "    return ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efa50184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d96305db36a462d852a702a109de64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokens:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5ca5f19a01429d918eeffd1e4245bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "looking for rhymes:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e7bc9e002f43d8828465dfad38d5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "looking for rhymes:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8e2b1eef304cfc9af5183b551a84d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "looking for rhymes:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2078720a835f4f289c3f9fb0bbfd6e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "looking for rhymes:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = '<|startoftext|>'\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "rhymes = []\n",
    "wersy_count = 0\n",
    "if len(re.findall('\\n', prompt)) > 0:\n",
    "    wersy_count += 1\n",
    "\n",
    "words_to_check = 100\n",
    "\n",
    "max_tokesn = 70\n",
    "\n",
    "for i in tqdm(range(max_tokesn), desc = 'tokens', \\\n",
    "              leave=True, \n",
    "              position=0):\n",
    "    next_token = get_next_token(generated)    \n",
    "    curr_word = tokenizer.decode(next_token[0])\n",
    "    \n",
    "    if '<|startoftext|>' in curr_word or '<|endoftext|>' in curr_word :\n",
    "        break\n",
    "    \n",
    "    #when we reached the end of line\n",
    "    elif curr_word == '\\n' and wersy_count < wersy_n:\n",
    "        wersy_count += 1\n",
    "    elif curr_word == '\\n' and wersy_count == wersy_n:\n",
    "        # generate most relevant words to finish sentence (up to 1k)\n",
    "        output_list = list(generated.squeeze().numpy())\n",
    "        # cut last 2 lines\n",
    "        last_2_lines = tokenizer.decode(output_list).split('\\n')[-2:]\n",
    "        \n",
    "        if len(last_2_lines[1]) < 1:\n",
    "            wersy_count = 0\n",
    "            next\n",
    "            \n",
    "        # cut last word\n",
    "        last_2_lines[1] = ' '.join(last_2_lines[1].split(' ')[:-1])\n",
    "        # get rhyme from first line\n",
    "        last_word_1st_line = re.sub(\"[^0-9a-zA-Z\\s]+\", '', last_2_lines[0]).strip().split(' ')[-1]\n",
    "        # join lines\n",
    "        last_2_lines_str = '\\n'.join(last_2_lines)+' '\n",
    "        tmp_generated = torch.tensor(tokenizer.encode(last_2_lines_str)).unsqueeze(0)\n",
    "        \n",
    "        next_token, possible_words = get_next_token(tmp_generated, words_to_check)    \n",
    "                \n",
    "        possible_endings = []\n",
    "        # generate given number of last words\n",
    "        for word_i in tqdm(range(words_to_check), desc = 'looking for rhymes', \n",
    "                           leave=True, \n",
    "                           position=1):\n",
    "            tmp_next_token  = torch.cat((tmp_generated, possible_words[word_i].reshape(1,1)), dim=1)\n",
    "            tmp_curr_word = tokenizer.decode(tmp_next_token[0][-1])\n",
    "            \n",
    "            counter = 0\n",
    "            while ~np.isin(tmp_curr_word, ['\\n']) and counter < 9:\n",
    "                tmp_next_token = torch.cat((tmp_next_token, get_next_token(tmp_next_token) ), dim=1)\n",
    "                tmp_curr_word = tokenizer.decode(tmp_next_token[0][-1])\n",
    "                counter += 1\n",
    "            \n",
    "            if counter < 9:\n",
    "                possible_endings.append(tokenizer.decode(tmp_next_token[0]))\n",
    "        \n",
    "        ending = get_last_word(last_word_1st_line, possible_endings).strip() # + '\\n'\n",
    "        \n",
    "        all_except_last_2_lines = '\\n'.join(tokenizer.decode(output_list).split('\\n')[:-2])\n",
    "        if len(all_except_last_2_lines) > 0:\n",
    "            all_except_last_2_lines = all_except_last_2_lines + '\\n'\n",
    "        \n",
    "        generated = torch.tensor(tokenizer.encode(all_except_last_2_lines+ending)).unsqueeze(0)\n",
    "        wersy_count = 0\n",
    "        next\n",
    "    \n",
    "    generated = torch.cat((generated, next_token), dim=1)\n",
    "    \n",
    "    if i > max_tokesn*0.75 and '.' in curr_word:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "WKWJqTkwh2Zu",
   "metadata": {
    "id": "WKWJqTkwh2Zu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wpadłem na Twą łąkę,\n",
      "Gdzieś w nią  Region warszawski,owiadywałem,\n",
      "Aż tu nagle olśnienie... rtużmałem,\n",
      "Gdzieś przed Tobą stała \n",
      "Śmierć i sława!  wogÓra przed Tobą stała\n",
      "W otchłań mrok, \n",
      "Jakieś groty,  mrok\n",
      "Jaki tajemny.\n"
     ]
    }
   ],
   "source": [
    "output_list = list(generated.squeeze().numpy())\n",
    "output_text = tokenizer.decode(output_list)\n",
    "print(re.sub('<\\|[a-z]+\\|>','',output_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "CWYq4coBpgYK",
   "metadata": {
    "id": "CWYq4coBpgYK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "06_finetune_papuGaPT2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
