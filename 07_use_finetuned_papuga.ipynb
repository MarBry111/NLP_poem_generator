{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "TIgckz6CYC3r",
   "metadata": {
    "id": "TIgckz6CYC3r"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389caad3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "389caad3",
    "outputId": "83480a68-89be-4f83-a7f9-040f6a9f911c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import re\n",
    "\n",
    "import pyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "935d8ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = pyphen.Pyphen(lang='pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7f38b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a7f38b4",
    "outputId": "857bfae6-41fb-43b1-d6be-8d6b2b9c09c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/marek/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:907: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('flax-community/papuGaPT2',\n",
    "                                          bos_token='<|startoftext|>', \n",
    "                                          eos_token='<|endoftext|>', \n",
    "                                          pad_token='<|pad|>')\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained('models/papuga-poems-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gaZQVH5mTo2x",
   "metadata": {
    "id": "gaZQVH5mTo2x"
   },
   "source": [
    "## Verify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Qb81QzYxR73j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qb81QzYxR73j",
    "outputId": "7b932bad-6a67-47bc-8432-05cbb7cc8b7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Litwo Ojczyzno moja ty jesteś jak zdrowie. Tak mi przytnę uszy Twe,\n",
      "Żeś mię sławił, g’woli Twojej wiernego sługi\n",
      "Jako żyw, jakoć mię w cale nie zawodzi,\n",
      "Jako żyw litości w sercu czuje.\n",
      "Wszytka rozkosz mą na tym świecie,\n",
      "Alem ja Ci, litościwy niechaj będzie dana\n",
      "I w Twoim kościele, i w Twoim domu\n",
      "Nie będę potym animował myśli naszych.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Litwo Ojczyzno moja ty jesteś jak zdrowie\"\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "    generated,\n",
    "    do_sample=True, \n",
    "    max_length=100, \n",
    "    top_k=50, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33h3Rh_MNqRq",
   "metadata": {
    "id": "33h3Rh_MNqRq"
   },
   "outputs": [],
   "source": [
    "model_g = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hYZLWwEGUcnY",
   "metadata": {
    "id": "hYZLWwEGUcnY"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "generated_num = 0\n",
    "generated_list = []\n",
    "\n",
    "filter_value = -float(\"Inf\")\n",
    "\n",
    "prompt = \"Litwo Ojczyzno moja ty jesteś jak zdrowie\\n\"\n",
    "\n",
    "entry_count=10\n",
    "entry_length=200\n",
    "top_p=0.75 #sort word probabilities descending order, sum upp to p dropping other words, keeping few\n",
    "temperature=1. # higher more originlal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "FYZuMsxmtEuI",
   "metadata": {
    "id": "FYZuMsxmtEuI"
   },
   "outputs": [],
   "source": [
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "outputs = model_g(generated, labels=generated)\n",
    "loss, logits = outputs[:2]\n",
    "logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "sorted_indices_to_remove = cumulative_probs > top_p\n",
    "sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "sorted_indices_to_remove[..., 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "r_qotM33ttva",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_qotM33ttva",
    "outputId": "47c72252-7653-45a3-fe5b-e47eac8f142b"
   },
   "outputs": [],
   "source": [
    "# logits_gr0_idx = (logits > 0).nonzero(as_tuple=True)[1]\n",
    "# logits_gr0_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab1d0f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits_gr0 = logits[(logits > 0)]\n",
    "# logits_gr0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ezcbgPsztSXK",
   "metadata": {
    "id": "ezcbgPsztSXK"
   },
   "outputs": [],
   "source": [
    "# f_softmax = F.softmax(logits_gr0, dim=-1)\n",
    "# f_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "219af465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_softmax = F.softmax(logits, dim=-1)\n",
    "indeces_sort = torch.argsort(f_softmax)[0,:50]\n",
    "indeces_sort.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d47150df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9, 16, 20, 19,  1, 18, 11,  8,  5, 23, 14,  3, 22, 17, 27, 12, 15, 25,\n",
       "         7,  6, 21, 13,  4, 26,  0, 24, 10,  2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indeces_sort = torch.argsort(f_softmax)\n",
    "indeces_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae12070",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "logits[:, indices_to_remove] = filter_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dWcX7vAdNmJG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "dWcX7vAdNmJG",
    "outputId": "f986522d-6986-4dc1-c5aa-1783e76cd1ec"
   },
   "outputs": [],
   "source": [
    "def get_next_token(generated, n_logits=50):\n",
    "    outputs = model_g(generated, labels=generated)\n",
    "    loss, logits = outputs[:2]\n",
    "    logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    \n",
    "    if True:\n",
    "        f_softmax = F.softmax(logits, dim=-1)\n",
    "        indeces_sort = torch.argsort(f_softmax)[0, :n_logits]\n",
    "    \n",
    "    logits[:, indices_to_remove] = filter_value\n",
    "    next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "        \n",
    "#     logits_gr0_idx = (logits > 0).nonzero(as_tuple=True)[1]\n",
    "#     logits_gr0 = logits[(logits > 0)]\n",
    "#     f_softmax = F.softmax(logits, dim=-1)\n",
    "#     indeces_sort = torch.argsort(f_softmax)[:n_logits]\n",
    "\n",
    "    return next_token, indeces_sort #, logits_gr0_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "af534482",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_r = 'aabb'\n",
    "if type_r == 'aabb':\n",
    "    wersy_n = 1\n",
    "else:\n",
    "    wersy_n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "efa50184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▉                                         | 4/60 [00:00<00:14,  3.82it/s]\n",
      "looking:   2%|▍                          | 163/10000 [00:00<00:00, 29055.31it/s]\u001b[A\n",
      "  8%|███▋                                        | 5/60 [00:01<00:14,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pomoże ['wie'] czowie wie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████▌                             | 19/60 [00:04<00:10,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ['słowa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████▍                          | 23/60 [00:05<00:10,  3.65it/s]\n",
      "looking:   0%|                                        | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
      "looking:  47%|████████████▏             | 4667/10000 [00:00<00:00, 46658.36it/s]\u001b[A\n",
      "looking: 100%|█████████████████████████| 10000/10000 [00:00<00:00, 46207.33it/s]\u001b[A\n",
      " 47%|████████████████████                       | 28/60 [00:07<00:10,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ['jedyna']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "looking:   0%|                                        | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
      "looking: 100%|█████████████████████████| 10000/10000 [00:00<00:00, 50176.68it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 36/60 [00:10<00:08,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ['Twoja']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████▍             | 41/60 [00:12<00:06,  2.96it/s]\n",
      "looking:   0%|                                        | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
      "looking: 100%|█████████████████████████| 10000/10000 [00:00<00:00, 52633.48it/s]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 43/60 [00:13<00:06,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ['bezpieczny,']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|███████████████████████████████████▊       | 50/60 [00:15<00:03,  2.71it/s]\n",
      "looking:   0%|                                        | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
      "looking:  45%|███████████▋              | 4477/10000 [00:00<00:00, 44763.53it/s]\u001b[A\n",
      "looking: 100%|█████████████████████████| 10000/10000 [00:00<00:00, 47453.32it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 52/60 [00:16<00:03,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ['ratował']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [00:19<00:00,  3.01it/s]\n"
     ]
    }
   ],
   "source": [
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "rhymes = []\n",
    "if len(re.findall('\\n', prompt)) > 0:\n",
    "    word = prompt.split(' ')[-1].replace('\\n', '')\n",
    "    rhymes.append(dic.inserted(word).split('-')[-1])\n",
    "\n",
    "looper_count = 10000\n",
    "prev_word = ''\n",
    "for i in tqdm(range(60)):\n",
    "    next_token, tokens_possible  = get_next_token(generated, looper_count)\n",
    "    generated_tmp = torch.cat((generated, next_token), dim=1)\n",
    "    next_token_tmp, _ = get_next_token(generated_tmp)\n",
    "    \n",
    "    curr_word = tokenizer.decode(next_token[0])\n",
    "    next_word = tokenizer.decode(next_token_tmp[0])\n",
    "    \n",
    "    looper = 0\n",
    "    \n",
    "    if '\\n' ==  next_word and len(rhymes) < wersy_n:\n",
    "        output_list = list(generated.squeeze().numpy())\n",
    "        word = tokenizer.decode(output_list).replace('\\n', ' ').strip().split(' ')[-1]\n",
    "        syllab = dic.inserted(re.sub('[,.-;?!]',' ',word)).split('-')[-1]            \n",
    "        rhymes.append(word)    \n",
    "        print('A', rhymes)\n",
    "    elif '\\n' ==  next_word and len(rhymes) == wersy_n:\n",
    "        output_list = list(generated.squeeze().numpy())\n",
    "        word = tokenizer.decode(output_list).replace('\\n', ' ').strip().split(' ')[-1]\n",
    "        syllab = dic.inserted(re.sub('[,.-;!?]',' ',word)).split('-')[-1]            \n",
    "        org_word = word\n",
    "\n",
    "        for looper in tqdm(range(tokens_possible.shape[0]),desc='looking'):\n",
    "            word = tokenizer.decode(tokens_possible[looper])\n",
    "            syllab = dic.inserted(word).split('-')[-1]\n",
    "            \n",
    "#             generated_tmp = torch.cat((generated, tokens_possible[looper].reshape(1,1)), dim=1)\n",
    "#             next_token_tmp, _ = get_next_token(generated_tmp)\n",
    "\n",
    "            next_word = tokenizer.decode(next_token_tmp[0])\n",
    "            if rhymes[0].lower() == syllab.lower() : # and next_word == '\\n':\n",
    "                break\n",
    "\n",
    "        if looper+1 < looper_count:\n",
    "            print(prev_word, rhymes,word,syllab)\n",
    "#             next_token = tokens_possible[looper].reshape(1,1)\n",
    "            next_token = torch.cat((tokens_possible[looper].reshape(1,1),\n",
    "                                    torch.tensor(tokenizer.encode('\\n')).unsqueeze(0)), dim=1)\n",
    "        rhymes = []\n",
    "        \n",
    "    prev_word = tokenizer.decode(next_token[0])\n",
    "\n",
    "    generated = torch.cat((generated, next_token), dim=1)\n",
    "    \n",
    "#     output_list = list(generated.squeeze().numpy())\n",
    "#     output_text = tokenizer.decode(output_list)\n",
    "#     print(output_text)\n",
    "    \n",
    "#     if curr_word == '\\n' and next_word == '\\n':\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "WKWJqTkwh2Zu",
   "metadata": {
    "id": "WKWJqTkwh2Zu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Litwo Ojczyzno moja ty jesteś jak zdrowie\n",
      "I łaska twoja pomożeczowie\n",
      "I leliją na każdy dzień\n",
      "I cisną się słowa: \"Dobra to rzecz,\n",
      "Moja jedyna.\"\n",
      "\n",
      "I taka jest wola Twoja:\n",
      "Żebych był bezpieczny, a\n",
      "Ale kto cię dziś będzie ratował,\n",
      "Kto dziś rano będzie spał na\n"
     ]
    }
   ],
   "source": [
    "output_list = list(generated.squeeze().numpy())\n",
    "output_text = tokenizer.decode(output_list)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "--L15D5dMuOo",
   "metadata": {
    "id": "--L15D5dMuOo"
   },
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30, #maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "              output_list = list(generated.squeeze().numpy())\n",
    "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
    "              generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list\n",
    "\n",
    "#Function to generate multiple sentences. Test data should be a dataframe\n",
    "def text_generation(test_data):\n",
    "  generated_lyrics = []\n",
    "  for i in range(len(test_data)):\n",
    "    x = generate(model.to('cpu'), tokenizer, test_data['Lyric'][i], entry_count=1)\n",
    "    generated_lyrics.append(x)\n",
    "  return generated_lyrics\n",
    "\n",
    "#Run the functions to generate the lyrics\n",
    "generated_lyrics = text_generation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "CWYq4coBpgYK",
   "metadata": {
    "id": "CWYq4coBpgYK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "06_finetune_papuGaPT2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
