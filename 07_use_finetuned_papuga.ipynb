{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "TIgckz6CYC3r",
   "metadata": {
    "id": "TIgckz6CYC3r"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "389caad3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "389caad3",
    "outputId": "83480a68-89be-4f83-a7f9-040f6a9f911c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import re\n",
    "\n",
    "import pyphen\n",
    "import jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "935d8ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = pyphen.Pyphen(lang='pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a7f38b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a7f38b4",
    "outputId": "857bfae6-41fb-43b1-d6be-8d6b2b9c09c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained('gpt2-xl',\n",
    "#                                           bos_token='<|startoftext|>', \n",
    "#                                           eos_token='<|endoftext|>', \n",
    "#                                           pad_token='<|pad|>')\n",
    "\n",
    "# model = AutoModelWithLMHead.from_pretrained('gpt2-xl')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('flax-community/papuGaPT2',\n",
    "                                          bos_token='<|startoftext|>', \n",
    "                                          eos_token='<|endoftext|>', \n",
    "                                          pad_token='<|pad|>')\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained('models/papuga-poems-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gaZQVH5mTo2x",
   "metadata": {
    "id": "gaZQVH5mTo2x"
   },
   "source": [
    "## Verify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "Qb81QzYxR73j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qb81QzYxR73j",
    "outputId": "7b932bad-6a67-47bc-8432-05cbb7cc8b7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Litwo Ojczyzno moja ty jesteś jak zdrowie\n",
      "Ja tobie ufam jako ręka Twego nieba\n",
      "I jako światło na każdą chwilę\n",
      "Radujmy się miłością wieczną\n",
      "Żeśmy szczęścia i radości mieli\n",
      "I niechaj nam szczęścia garć\n",
      "A niechaj trwa pieśń ta trwa pamięć ta\n",
      "Niechaj zakwitną w wiecznej naszej pamięci\n",
      "Litwa i Polska będą królestwem swoim\n",
      "I na zawsze na wieki wieczną Polską\n",
      "\n",
      "I na wieki szczęście i Polska okrągłym słowie\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Litwo Ojczyzno moja ty jesteś jak zdrowie\\n\"\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "    generated,\n",
    "    do_sample=True, \n",
    "    max_length=100, \n",
    "    top_k=50, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33h3Rh_MNqRq",
   "metadata": {
    "id": "33h3Rh_MNqRq"
   },
   "outputs": [],
   "source": [
    "model_g = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "hYZLWwEGUcnY",
   "metadata": {
    "id": "hYZLWwEGUcnY"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "generated_num = 0\n",
    "generated_list = []\n",
    "\n",
    "filter_value = -float(\"Inf\")\n",
    "\n",
    "prompt = \"Litwo Ojczyzno moja ty jesteś jak zdrowie\\n\"\n",
    "\n",
    "entry_count=10\n",
    "entry_length=200\n",
    "top_p=0.75 #sort word probabilities descending order, sum upp to p dropping other words, keeping few\n",
    "temperature=1. # higher more originlal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "FYZuMsxmtEuI",
   "metadata": {
    "id": "FYZuMsxmtEuI"
   },
   "outputs": [],
   "source": [
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "outputs = model_g(generated, labels=generated)\n",
    "loss, logits = outputs[:2]\n",
    "logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "sorted_indices_to_remove = cumulative_probs > top_p\n",
    "sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "sorted_indices_to_remove[..., 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "r_qotM33ttva",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_qotM33ttva",
    "outputId": "47c72252-7653-45a3-fe5b-e47eac8f142b"
   },
   "outputs": [],
   "source": [
    "# logits_gr0_idx = (logits > 0).nonzero(as_tuple=True)[1]\n",
    "# logits_gr0_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab1d0f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits_gr0 = logits[(logits > 0)]\n",
    "# logits_gr0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ezcbgPsztSXK",
   "metadata": {
    "id": "ezcbgPsztSXK"
   },
   "outputs": [],
   "source": [
    "# f_softmax = F.softmax(logits_gr0, dim=-1)\n",
    "# f_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "219af465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_softmax = F.softmax(logits, dim=-1)\n",
    "indeces_sort = torch.argsort(f_softmax)[0,:50]\n",
    "indeces_sort.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d47150df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21007, 34024, 39868,  ..., 28667,   956,    45]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indeces_sort = torch.argsort(f_softmax)\n",
    "indeces_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ae12070",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "logits[:, indices_to_remove] = filter_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dWcX7vAdNmJG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "dWcX7vAdNmJG",
    "outputId": "f986522d-6986-4dc1-c5aa-1783e76cd1ec"
   },
   "outputs": [],
   "source": [
    "def get_next_token(generated, n_logits=None):\n",
    "    outputs = model_g(generated, labels=generated)\n",
    "    loss, logits = outputs[:2]\n",
    "    logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    \n",
    "    if n_logits:\n",
    "        f_softmax = F.softmax(logits, dim=-1)\n",
    "        indeces_sort = torch.argsort(f_softmax)[0, :n_logits]\n",
    "    \n",
    "    logits[:, indices_to_remove] = filter_value\n",
    "    next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "        \n",
    "#     logits_gr0_idx = (logits > 0).nonzero(as_tuple=True)[1]\n",
    "#     logits_gr0 = logits[(logits > 0)]\n",
    "#     f_softmax = F.softmax(logits, dim=-1)\n",
    "#     indeces_sort = torch.argsort(f_softmax)[:n_logits]\n",
    "    \n",
    "    if n_logits:\n",
    "        return next_token, indeces_sort #, logits_gr0_idx\n",
    "    else:\n",
    "        return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "af534482",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_r = 'aabb'\n",
    "if type_r == 'aabb':\n",
    "    wersy_n = 1\n",
    "else:\n",
    "    wersy_n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b84ab6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_word(prev_word, possible_endings):\n",
    "    prev_syllab = dic.inserted(prev_word).split('-')[-1]\n",
    "\n",
    "    simmilarities = []\n",
    "    possible_endings = np.array(possible_endings)\n",
    "\n",
    "    for p in possible_endings:\n",
    "        last_word = re.sub(\"[^0-9a-zA-Z\\s]+\", '', p).strip().split(' ')[-1]\n",
    "        last_syllab = dic.inserted(last_word).split('-')[-1]\n",
    "        \n",
    "#         if last_word[-4:] == prev_word[-4:]:\n",
    "#             simmilarities.append(0)\n",
    "#         elif last_word[-3:] == prev_word[-3:]:\n",
    "#             simmilarities.append(0.33)        \n",
    "#         elif last_word[-2:] == prev_word[-2:]:\n",
    "#             simmilarities.append(0.66)\n",
    "#         else:\n",
    "#             simmilarities.append(1)\n",
    "        min_l = np.min( [len(last_syllab), len(prev_syllab)] )\n",
    "            \n",
    "        if min_l > 1:\n",
    "            prev_syllab_tmp = prev_word[-min_l:].lower()\n",
    "            last_syllab_tmp = last_word[-min_l:].lower()\n",
    "            s = jellyfish.levenshtein_distance(prev_syllab_tmp, last_syllab_tmp)\n",
    "            simmilarities.append(s/min_l)\n",
    "#                 s = 0\n",
    "#                 for e in range(np.min( [len(last_word), len(prev_word)] )):\n",
    "#                     if last_word[-e] == prev_word[-e]:\n",
    "#                         s += 1\n",
    "#                     else:\n",
    "#                         break                 \n",
    "#                 simmilarities.append(s)\n",
    "        else:\n",
    "            simmilarities.append(10)\n",
    "\n",
    "    choose_endings = possible_endings[np.min(simmilarities) == np.array(simmilarities)]\n",
    "#     choose_endings = possible_endings[np.max(simmilarities) == np.array(simmilarities)]\n",
    "    ending = choose_endings[np.random.randint(0,choose_endings.shape[0])]\n",
    "    \n",
    "    return ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "efa50184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff9370edcdc4f659975c28ed868cfca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokens:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5b5832eabb4c348d878893ca3b7022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "looking for rhymes:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4237add817914d71857f20b50b97415e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "looking for rhymes:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443b98b4e94b496ea21c396f3dfa778d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "looking for rhymes:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936eb0a60ed04058b21330d90c59d75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "looking for rhymes:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "rhymes = []\n",
    "wersy_count = 0\n",
    "if len(re.findall('\\n', prompt)) > 0:\n",
    "    wersy_count += 1\n",
    "\n",
    "words_to_check = 100\n",
    "\n",
    "for i in tqdm(range(50), desc = 'tokens', \\\n",
    "              leave=True, \n",
    "              position=0):\n",
    "    next_token = get_next_token(generated)    \n",
    "    curr_word = tokenizer.decode(next_token[0])\n",
    "    \n",
    "    if curr_word == '<|startoftext|>' or curr_word == '<|endoftext|>' :\n",
    "        break\n",
    "    \n",
    "    #when we reached the end of line\n",
    "    elif curr_word == '\\n' and wersy_count < wersy_n:\n",
    "        wersy_count += 1\n",
    "    elif curr_word == '\\n' and wersy_count == wersy_n:\n",
    "        # generate most relevant words to finish sentence (up to 1k)\n",
    "        output_list = list(generated.squeeze().numpy())\n",
    "        # cut last 2 lines\n",
    "        last_2_lines = tokenizer.decode(output_list).split('\\n')[-2:]\n",
    "        \n",
    "        if len(last_2_lines[1]) < 1:\n",
    "            wersy_count = 0\n",
    "            next\n",
    "            \n",
    "        # cut last word\n",
    "        last_2_lines[1] = ' '.join(last_2_lines[1].split(' ')[:-1])\n",
    "        # get rhyme from first line\n",
    "        last_word_1st_line = re.sub(\"[^0-9a-zA-Z\\s]+\", '', last_2_lines[0]).strip().split(' ')[-1]\n",
    "        # join lines\n",
    "        last_2_lines_str = '\\n'.join(last_2_lines)+' '\n",
    "        tmp_generated = torch.tensor(tokenizer.encode(last_2_lines_str)).unsqueeze(0)\n",
    "        \n",
    "        next_token, possible_words = get_next_token(tmp_generated, words_to_check)    \n",
    "                \n",
    "        possible_endings = []\n",
    "        # generate given number of last words\n",
    "        for word_i in tqdm(range(words_to_check), desc = 'looking for rhymes', \n",
    "                           leave=True, \n",
    "                           position=1):\n",
    "            tmp_next_token  = torch.cat((tmp_generated, possible_words[word_i].reshape(1,1)), dim=1)\n",
    "            tmp_curr_word = tokenizer.decode(tmp_next_token[0][-1])\n",
    "            \n",
    "            counter = 0\n",
    "            while ~np.isin(tmp_curr_word, ['\\n']) and counter < 9:\n",
    "                tmp_next_token = torch.cat((tmp_next_token, get_next_token(tmp_next_token) ), dim=1)\n",
    "                tmp_curr_word = tokenizer.decode(tmp_next_token[0][-1])\n",
    "                counter += 1\n",
    "            \n",
    "            if counter < 9:\n",
    "                possible_endings.append(tokenizer.decode(tmp_next_token[0]))\n",
    "        \n",
    "        ending = get_last_word(last_word_1st_line, possible_endings).strip() # + '\\n'\n",
    "        \n",
    "        all_except_last_2_lines = '\\n'.join(tokenizer.decode(output_list).split('\\n')[:-2])\n",
    "        if len(all_except_last_2_lines) > 0:\n",
    "            all_except_last_2_lines = all_except_last_2_lines + '\\n'\n",
    "        \n",
    "        generated = torch.tensor(tokenizer.encode(all_except_last_2_lines+ending)).unsqueeze(0)\n",
    "        wersy_count = 0\n",
    "        next\n",
    "    \n",
    "    generated = torch.cat((generated, next_token), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "WKWJqTkwh2Zu",
   "metadata": {
    "id": "WKWJqTkwh2Zu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Litwo Ojczyzno moja ty jesteś jak zdrowie\n",
      "Imię twe w sercu <|pad|> tobie,łka to w radości\n",
      "I na co? I na co? I na tniczychłowy\n",
      "Rolls-Royce<|pad|>  postem nabiałowy.\n",
      "I gdziekolwiek ty zmulisz swoje dary,\n",
      "Tam oddasz chwałę J\n"
     ]
    }
   ],
   "source": [
    "output_list = list(generated.squeeze().numpy())\n",
    "output_text = tokenizer.decode(output_list)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "CWYq4coBpgYK",
   "metadata": {
    "id": "CWYq4coBpgYK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "06_finetune_papuGaPT2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
